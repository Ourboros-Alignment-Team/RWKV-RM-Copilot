[2025-04-09 19:34:19,189] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2025-04-09 19:34:19,189] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2025-04-09 19:34:19,189] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2025-04-09 19:34:19,190] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2025-04-09 19:34:19,190] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
Epoch 0, Loss: 0.98828125, Positive Score: 0.57421875, Negative Score: 0.5859375
/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 0, Loss: 0.97265625, Positive Score: 0.5546875, Negative Score: 0.58203125
Epoch 0, Loss: 0.92578125, Positive Score: 0.546875, Negative Score: 0.62109375
Epoch 0, Loss: 0.9375, Positive Score: 0.6328125, Negative Score: 0.6953125
Epoch 0, Loss: 0.87890625, Positive Score: 0.53515625, Negative Score: 0.65625
Epoch 0, Loss: 0.80859375, Positive Score: 0.56640625, Negative Score: 0.7578125
Epoch 0, Loss: 0.5234375, Positive Score: 0.248046875, Negative Score: 0.72265625
Epoch 0, Loss: 1.015625, Positive Score: 0.80078125, Negative Score: 0.78125
Epoch 0, Loss: 0.84375, Positive Score: 0.69140625, Negative Score: 0.84765625
Epoch 0, Loss: 0.90234375, Positive Score: 0.54296875, Negative Score: 0.640625
[2025-04-09 19:34:38,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 19:34:38,606] [INFO] [timer.py:215:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.5062340867550703, CurrSamplesPerSec=0.6452444820903791, MemAllocated=5.02GB, MaxMemAllocated=15.59GB
Epoch 0, Loss: 0.61328125, Positive Score: 0.52734375, Negative Score: 0.9140625
Epoch 0, Loss: 0.671875, Positive Score: 0.3203125, Negative Score: 0.6484375
Epoch 0, Loss: 0.796875, Positive Score: 0.50390625, Negative Score: 0.70703125
Epoch 0, Loss: 0.92578125, Positive Score: 0.74609375, Negative Score: 0.8203125
Epoch 0, Loss: 0.30859375, Positive Score: 0.1640625, Negative Score: 0.85546875
Epoch 1, Loss: 0.66796875, Positive Score: 0.4765625, Negative Score: 0.80859375
Epoch 1, Loss: 0.3046875, Positive Score: 0.05615234375, Negative Score: 0.75
Epoch 1, Loss: 0.51953125, Positive Score: 0.1630859375, Negative Score: 0.64453125
Epoch 1, Loss: 0.5859375, Positive Score: 0.349609375, Negative Score: 0.76171875
Epoch 1, Loss: 0.48828125, Positive Score: 0.484375, Negative Score: 0.99609375
[2025-04-09 19:35:03,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 19:35:03,074] [INFO] [timer.py:215:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.5109663859964045, CurrSamplesPerSec=0.44696753336987816, MemAllocated=5.02GB, MaxMemAllocated=15.59GB
Epoch 1, Loss: 0.81640625, Positive Score: 0.81640625, Negative Score: 1.0
Epoch 1, Loss: 0.5703125, Positive Score: 0.5703125, Negative Score: 1.0
Epoch 1, Loss: 0.4375, Positive Score: 0.4375, Negative Score: 1.0
Epoch 1, Loss: 0.27734375, Positive Score: 0.2734375, Negative Score: 0.99609375
Epoch 1, Loss: 0.2890625, Positive Score: 0.263671875, Negative Score: 0.97265625
Epoch 1, Loss: 0.5859375, Positive Score: 0.259765625, Negative Score: 0.67578125
Epoch 1, Loss: 0.75, Positive Score: 0.291015625, Negative Score: 0.54296875
Traceback (most recent call last):
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/train_model.py", line 255, in <module>
    model_engine.backward(Loss)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1862, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1901, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/function.py", line 289, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 658, in backward
    outputs = ctx.run_function(*detached_inputs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/model/block.py", line 27, in forward
    x_attn, v_first = self.att(
                      ^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/model/att.py", line 159, in forward
    x = RUN_CUDA_RWKV7g(r, w, k, v, -kk, kk * a)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/model/rwkvop.py", line 57, in RUN_CUDA_RWKV7g
    return WindBackstepping.apply(w, q, k, v, a, b).view(B, T, HC)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/function.py", line 553, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/model/rwkvop.py", line 34, in forward
    s = torch.empty(
        ^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.12 GiB. GPU 0 has a total capacity of 23.68 GiB of which 5.29 GiB is free. Including non-PyTorch memory, this process has 18.22 GiB memory in use. Of the allocated memory 10.96 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)