[2025-04-09 01:59:05,274] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2025-04-09 01:59:05,274] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2025-04-09 01:59:05,274] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2025-04-09 01:59:05,274] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2025-04-09 01:59:05,274] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
Epoch 0, Loss: -1.9921875
/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1829: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
Epoch 0, Loss: -1.921875
Epoch 0, Loss: -1.59375
Epoch 0, Loss: -1.8984375
Epoch 0, Loss: -1.7578125
Epoch 0, Loss: -1.6796875
Epoch 0, Loss: -1.6640625
Epoch 0, Loss: -1.828125
Epoch 0, Loss: -1.78125
Epoch 0, Loss: -1.78125
[2025-04-09 01:59:22,224] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 01:59:22,225] [INFO] [timer.py:215:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.5973687592623322, CurrSamplesPerSec=0.4457382267323231, MemAllocated=5.52GB, MaxMemAllocated=7.09GB
Epoch 0, Loss: -1.8671875
Epoch 0, Loss: -1.859375
Epoch 0, Loss: -1.8125
Epoch 0, Loss: -1.9765625
Epoch 0, Loss: -1.90625
Epoch 0, Loss: -1.828125
Epoch 0, Loss: -1.3046875
Epoch 0, Loss: -1.640625
Epoch 0, Loss: -1.953125
Epoch 0, Loss: -1.84375
[2025-04-09 01:59:38,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=20, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 01:59:38,819] [INFO] [timer.py:215:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.6004636720993984, CurrSamplesPerSec=0.444665052814195, MemAllocated=5.52GB, MaxMemAllocated=7.09GB
Epoch 0, Loss: -1.75
Epoch 0, Loss: -1.921875
Epoch 0, Loss: -1.7890625
Epoch 0, Loss: -1.703125
Epoch 0, Loss: -2.0
Epoch 0, Loss: -1.828125
Epoch 0, Loss: -1.765625
Epoch 0, Loss: -1.484375
Epoch 0, Loss: -1.890625
Epoch 0, Loss: -1.5625
[2025-04-09 01:59:56,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=30, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 01:59:56,580] [INFO] [timer.py:215:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.5866841189777418, CurrSamplesPerSec=0.7300852522501933, MemAllocated=5.52GB, MaxMemAllocated=7.67GB
Epoch 0, Loss: -1.6484375
Epoch 0, Loss: -1.7265625
Epoch 0, Loss: -1.7109375
Epoch 0, Loss: -1.578125
Epoch 0, Loss: -1.6640625
Epoch 0, Loss: -1.6875
Epoch 0, Loss: -1.8125
Epoch 0, Loss: -1.84375
Epoch 0, Loss: -1.6640625
Epoch 0, Loss: -1.984375
[2025-04-09 02:00:13,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=40, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:00:13,860] [INFO] [timer.py:215:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=0.5846526473280099, CurrSamplesPerSec=0.5680984762682346, MemAllocated=5.52GB, MaxMemAllocated=7.67GB
Epoch 0, Loss: -1.625
Epoch 0, Loss: -1.6015625
Epoch 0, Loss: -1.8984375
Epoch 0, Loss: -1.6796875
Epoch 0, Loss: -1.765625
Epoch 0, Loss: -1.65625
Epoch 0, Loss: -1.4921875
Epoch 0, Loss: -1.625
Epoch 0, Loss: -1.875
Epoch 0, Loss: -1.8671875
[2025-04-09 02:00:36,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=50, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:00:36,671] [INFO] [timer.py:215:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=0.5467374230136928, CurrSamplesPerSec=0.4582489929686712, MemAllocated=5.52GB, MaxMemAllocated=13.24GB
Epoch 0, Loss: -1.734375
Epoch 0, Loss: -1.796875
Epoch 0, Loss: -1.921875
Epoch 0, Loss: -1.71875
Epoch 0, Loss: -1.4453125
Epoch 0, Loss: -1.8125
Epoch 0, Loss: -1.6328125
Epoch 0, Loss: -1.765625
Epoch 0, Loss: -1.859375
Epoch 0, Loss: -1.703125
[2025-04-09 02:00:53,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=60, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:00:53,634] [INFO] [timer.py:215:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=0.5537113777251247, CurrSamplesPerSec=0.7356147664399475, MemAllocated=5.52GB, MaxMemAllocated=13.24GB
Epoch 0, Loss: -1.40625
Epoch 0, Loss: -1.84375
Epoch 0, Loss: -1.6796875
Epoch 0, Loss: -2.0625
Epoch 0, Loss: -1.75
Epoch 0, Loss: -1.6171875
Epoch 1, Loss: -1.71875
Epoch 1, Loss: -1.625
Epoch 1, Loss: -1.640625
Epoch 1, Loss: -1.8671875
[2025-04-09 02:01:09,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=70, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:01:09,887] [INFO] [timer.py:215:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=0.5623120005582177, CurrSamplesPerSec=0.7379054469273497, MemAllocated=5.52GB, MaxMemAllocated=13.24GB
Epoch 1, Loss: -1.734375
Epoch 1, Loss: -1.8125
Epoch 1, Loss: -1.6171875
Epoch 1, Loss: -1.734375
Epoch 1, Loss: -2.140625
Epoch 1, Loss: -1.875
Epoch 1, Loss: -1.7578125
Epoch 1, Loss: -1.953125
Epoch 1, Loss: -1.578125
Epoch 1, Loss: -1.734375
[2025-04-09 02:01:28,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=80, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:01:28,127] [INFO] [timer.py:215:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=0.5605107983556773, CurrSamplesPerSec=0.5062263500563399, MemAllocated=5.52GB, MaxMemAllocated=13.24GB
Epoch 1, Loss: -1.8125
Epoch 1, Loss: -1.828125
Epoch 1, Loss: -1.828125
Epoch 1, Loss: -1.75
Epoch 1, Loss: -1.5859375
Epoch 1, Loss: -1.6640625
Epoch 1, Loss: -1.90625
Epoch 1, Loss: -1.7265625
Epoch 1, Loss: -1.75
Epoch 1, Loss: -1.953125
[2025-04-09 02:01:45,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=90, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:01:45,986] [INFO] [timer.py:215:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=0.5604817226557988, CurrSamplesPerSec=0.8001217836729757, MemAllocated=5.52GB, MaxMemAllocated=13.24GB
Epoch 1, Loss: -1.828125
Epoch 1, Loss: -1.9296875
Epoch 1, Loss: -1.75
Epoch 1, Loss: -1.640625
Epoch 1, Loss: -1.8984375
Epoch 1, Loss: -1.8515625
Epoch 1, Loss: -1.84375
Epoch 1, Loss: -1.546875
Epoch 1, Loss: -1.421875
Epoch 1, Loss: -1.8359375
[2025-04-09 02:02:03,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=100, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:02:03,767] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=0.5607120331896752, CurrSamplesPerSec=0.5646548201858218, MemAllocated=5.52GB, MaxMemAllocated=13.24GB
Epoch 1, Loss: -1.9140625
Epoch 1, Loss: -1.7421875
Epoch 1, Loss: -1.859375
Epoch 1, Loss: -1.7421875
Epoch 1, Loss: -1.796875
Epoch 1, Loss: -1.8515625
Epoch 1, Loss: -1.8359375
Epoch 1, Loss: -2.125
Epoch 1, Loss: -1.9296875
Epoch 1, Loss: -1.859375
[2025-04-09 02:02:20,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=110, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:02:20,638] [INFO] [timer.py:215:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=0.5635597022021959, CurrSamplesPerSec=0.6886244336581269, MemAllocated=5.52GB, MaxMemAllocated=13.24GB
Epoch 1, Loss: -1.5625
Epoch 1, Loss: -1.734375
Epoch 1, Loss: -1.5
Epoch 1, Loss: -1.6484375
Epoch 1, Loss: -1.640625
Epoch 1, Loss: -1.96875
Epoch 1, Loss: -1.8125
Epoch 1, Loss: -1.828125
Epoch 1, Loss: -1.7265625
Epoch 1, Loss: -1.8671875
[2025-04-09 02:02:36,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=120, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:02:36,644] [INFO] [timer.py:215:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=0.5683003322802361, CurrSamplesPerSec=0.7310105706506063, MemAllocated=5.52GB, MaxMemAllocated=13.24GB
Epoch 1, Loss: -1.9375
Epoch 1, Loss: -1.671875
Epoch 1, Loss: -2.078125
Epoch 1, Loss: -1.6875
Epoch 1, Loss: -1.78125
Epoch 1, Loss: -1.3828125
Epoch 1, Loss: -1.8671875
Epoch 1, Loss: -1.8125
Epoch 1, Loss: -1.859375
Epoch 1, Loss: -1.734375
[2025-04-09 02:02:56,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=130, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:02:56,716] [INFO] [timer.py:215:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=0.5621501687339759, CurrSamplesPerSec=0.7359606898438293, MemAllocated=5.52GB, MaxMemAllocated=13.24GB
Epoch 1, Loss: -1.796875
Epoch 1, Loss: -1.6328125
Epoch 2, Loss: -1.953125
Epoch 2, Loss: -1.828125
Epoch 2, Loss: -1.9375
Epoch 2, Loss: -1.625
Epoch 2, Loss: -1.7734375
Epoch 2, Loss: -1.671875
Epoch 2, Loss: -1.671875
Epoch 2, Loss: -1.96875
[2025-04-09 02:03:14,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=140, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:03:14,628] [INFO] [timer.py:215:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=0.5620980721171904, CurrSamplesPerSec=0.600770947825808, MemAllocated=5.52GB, MaxMemAllocated=13.24GB
Epoch 2, Loss: -1.828125
Epoch 2, Loss: -1.59375
Epoch 2, Loss: -1.7265625
Epoch 2, Loss: -1.75
Epoch 2, Loss: -1.625
Epoch 2, Loss: -1.921875
Epoch 2, Loss: -1.828125
Epoch 2, Loss: -1.96875
Epoch 2, Loss: -1.953125
Epoch 2, Loss: -1.859375
[2025-04-09 02:03:29,703] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=150, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:03:29,704] [INFO] [timer.py:215:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=0.5679714638466831, CurrSamplesPerSec=0.5784496108849873, MemAllocated=5.52GB, MaxMemAllocated=13.24GB
Epoch 2, Loss: -1.59375
Epoch 2, Loss: -1.7734375
Epoch 2, Loss: -1.5703125
Epoch 2, Loss: -1.875
Epoch 2, Loss: -1.7890625
Epoch 2, Loss: -1.8359375
Epoch 2, Loss: -1.859375
Epoch 2, Loss: -1.6953125
Epoch 2, Loss: -1.8046875
Epoch 2, Loss: -1.75
[2025-04-09 02:03:46,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=160, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:03:46,099] [INFO] [timer.py:215:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=0.5704726737925269, CurrSamplesPerSec=0.7833397175064111, MemAllocated=5.52GB, MaxMemAllocated=13.24GB
Epoch 2, Loss: -1.984375
Epoch 2, Loss: -1.703125
Epoch 2, Loss: -1.8125
