[2025-04-09 03:26:33,578] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2025-04-09 03:26:33,578] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2025-04-09 03:26:33,578] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2025-04-09 03:26:33,578] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2025-04-09 03:26:33,578] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
Epoch 0, Loss: 1.015625, Positive Score: -0.051025390625, Negative Score: -0.033935546875
/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 0, Loss: 0.9609375, Positive Score: -0.0390625, Negative Score: -0.076171875
Epoch 0, Loss: 1.015625, Positive Score: -0.0732421875, Negative Score: -0.0546875
Epoch 0, Loss: 1.0078125, Positive Score: -0.08251953125, Negative Score: -0.072265625
Epoch 0, Loss: 0.97265625, Positive Score: -0.11376953125, Negative Score: -0.1396484375
Epoch 0, Loss: 1.0234375, Positive Score: -0.076171875, Negative Score: -0.05126953125
Epoch 0, Loss: 0.9296875, Positive Score: 0.03076171875, Negative Score: -0.041748046875
Epoch 0, Loss: 1.0, Positive Score: -0.10693359375, Negative Score: -0.1083984375
Epoch 0, Loss: 0.9609375, Positive Score: -0.11767578125, Negative Score: -0.158203125
Epoch 0, Loss: 1.03125, Positive Score: 0.017333984375, Negative Score: 0.05126953125
[2025-04-09 03:27:00,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 03:27:00,263] [INFO] [timer.py:215:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.3804353185688673, CurrSamplesPerSec=0.2329735687480778, MemAllocated=5.79GB, MaxMemAllocated=14.62GB
Epoch 1, Loss: 0.99609375, Positive Score: -0.03564453125, Negative Score: -0.04052734375
Epoch 1, Loss: 0.921875, Positive Score: -0.021484375, Negative Score: -0.0986328125
Epoch 1, Loss: 0.97265625, Positive Score: 0.031982421875, Negative Score: 0.005096435546875
Traceback (most recent call last):
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/train_model.py", line 241, in <module>
    model_engine.backward(Loss)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1862, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1901, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/function.py", line 289, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 684, in backward
    torch.autograd.backward(output_tensors, grad_tensors)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 840.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 232.81 MiB is free. Including non-PyTorch memory, this process has 23.30 GiB memory in use. Of the allocated memory 21.26 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)