[2025-04-09 19:39:46,112] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2025-04-09 19:39:46,112] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2025-04-09 19:39:46,112] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2025-04-09 19:39:46,112] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2025-04-09 19:39:46,112] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
Epoch 0, Loss: 1.015625, Positive Score: 0.6796875, Negative Score: 0.6953125
/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 0, Loss: 1.0078125, Positive Score: 0.6875, Negative Score: 0.6953125
Epoch 0, Loss: 1.015625, Positive Score: 0.68359375, Negative Score: 0.69921875
Epoch 0, Loss: 1.0, Positive Score: 0.66796875, Negative Score: 0.66796875
Epoch 0, Loss: 0.94921875, Positive Score: 0.6171875, Negative Score: 0.56640625
Epoch 0, Loss: 0.9140625, Positive Score: 0.50390625, Negative Score: 0.416015625
Epoch 0, Loss: 0.99609375, Positive Score: 0.51171875, Negative Score: 0.5078125
Epoch 0, Loss: 0.9375, Positive Score: 0.380859375, Negative Score: 0.318359375
Epoch 0, Loss: 0.87109375, Positive Score: 0.70703125, Negative Score: 0.578125
Epoch 0, Loss: 0.9765625, Positive Score: 0.48046875, Negative Score: 0.458984375
[2025-04-09 19:40:04,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 19:40:04,314] [INFO] [timer.py:215:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.5486690523668549, CurrSamplesPerSec=0.5182384087156588, MemAllocated=5.02GB, MaxMemAllocated=11.87GB
Epoch 0, Loss: 0.92578125, Positive Score: 0.43359375, Negative Score: 0.359375
Epoch 0, Loss: 1.03125, Positive Score: 0.41796875, Negative Score: 0.4453125
Epoch 0, Loss: 0.8671875, Positive Score: 0.65234375, Negative Score: 0.51953125
Epoch 0, Loss: 0.8515625, Positive Score: 0.470703125, Negative Score: 0.322265625
Epoch 0, Loss: 0.79296875, Positive Score: 0.765625, Negative Score: 0.55859375
Epoch 1, Loss: 0.79296875, Positive Score: 0.8359375, Negative Score: 0.62890625
Epoch 1, Loss: 0.6640625, Positive Score: 0.921875, Negative Score: 0.5859375
Epoch 1, Loss: 0.73046875, Positive Score: 0.96484375, Negative Score: 0.6953125
Epoch 1, Loss: 0.65234375, Positive Score: 0.90625, Negative Score: 0.55859375
Epoch 1, Loss: 0.7109375, Positive Score: 0.62890625, Negative Score: 0.341796875
[2025-04-09 19:40:30,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 19:40:30,704] [INFO] [timer.py:215:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.5121122711411484, CurrSamplesPerSec=0.49080589659256313, MemAllocated=5.02GB, MaxMemAllocated=16.18GB
Epoch 1, Loss: 0.953125, Positive Score: 0.1435546875, Negative Score: 0.09521484375
Epoch 1, Loss: 0.94921875, Positive Score: 0.1943359375, Negative Score: 0.142578125
Epoch 1, Loss: 0.7109375, Positive Score: 0.3125, Negative Score: 0.02392578125
Epoch 1, Loss: 0.73046875, Positive Score: 0.478515625, Negative Score: 0.208984375
Epoch 1, Loss: 0.9296875, Positive Score: 0.78125, Negative Score: 0.7109375
Epoch 1, Loss: 0.82421875, Positive Score: 0.79296875, Negative Score: 0.6171875
Epoch 1, Loss: 0.7734375, Positive Score: 0.72265625, Negative Score: 0.49609375
Epoch 1, Loss: 0.4921875, Positive Score: 0.6484375, Negative Score: 0.1416015625
Epoch 1, Loss: 0.6875, Positive Score: 0.3203125, Negative Score: 0.01025390625
Epoch 1, Loss: 0.796875, Positive Score: 0.38671875, Negative Score: 0.185546875
[2025-04-09 19:40:49,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 19:40:49,659] [INFO] [timer.py:215:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.5180712536585479, CurrSamplesPerSec=0.28500436003321683, MemAllocated=5.02GB, MaxMemAllocated=16.18GB
Epoch 1, Loss: 0.9140625, Positive Score: 0.451171875, Negative Score: 0.3671875
Epoch 2, Loss: 0.79296875, Positive Score: 0.65234375, Negative Score: 0.4453125
Epoch 2, Loss: 0.65625, Positive Score: 0.828125, Negative Score: 0.486328125
Epoch 2, Loss: 0.5078125, Positive Score: 0.9375, Negative Score: 0.443359375
Epoch 2, Loss: 0.515625, Positive Score: 0.9765625, Negative Score: 0.4921875
Epoch 2, Loss: 0.8125, Positive Score: 0.87890625, Negative Score: 0.69140625
Epoch 2, Loss: 0.6875, Positive Score: 0.79296875, Negative Score: 0.478515625
Epoch 2, Loss: 0.87109375, Positive Score: 0.94140625, Negative Score: 0.8125
Epoch 2, Loss: 0.734375, Positive Score: 0.765625, Negative Score: 0.5
Epoch 2, Loss: 0.35546875, Positive Score: 0.96484375, Negative Score: 0.3203125
[2025-04-09 19:41:09,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 19:41:09,593] [INFO] [timer.py:215:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=0.515085627586906, CurrSamplesPerSec=0.7699293346412984, MemAllocated=5.02GB, MaxMemAllocated=16.18GB
Epoch 2, Loss: 0.375, Positive Score: 0.7265625, Negative Score: 0.10205078125
Epoch 2, Loss: 0.46484375, Positive Score: 0.65625, Negative Score: 0.12060546875
Epoch 2, Loss: 0.4453125, Positive Score: 0.6640625, Negative Score: 0.10888671875
Epoch 2, Loss: 0.73828125, Positive Score: 0.515625, Negative Score: 0.25390625
Epoch 2, Loss: 0.74609375, Positive Score: 0.7265625, Negative Score: 0.47265625
Epoch 2, Loss: 0.46875, Positive Score: 0.61328125, Negative Score: 0.083984375
Traceback (most recent call last):
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/train_model.py", line 255, in <module>
    model_engine.backward(Loss)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1862, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1901, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/function.py", line 289, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 658, in backward
    outputs = ctx.run_function(*detached_inputs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/model/block.py", line 27, in forward
    x_attn, v_first = self.att(
                      ^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/model/att.py", line 159, in forward
    x = RUN_CUDA_RWKV7g(r, w, k, v, -kk, kk * a)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/model/rwkvop.py", line 57, in RUN_CUDA_RWKV7g
    return WindBackstepping.apply(w, q, k, v, a, b).view(B, T, HC)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/function.py", line 553, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/model/rwkvop.py", line 34, in forward
    s = torch.empty(
        ^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacity of 23.68 GiB of which 6.32 GiB is free. Including non-PyTorch memory, this process has 17.20 GiB memory in use. Of the allocated memory 10.34 GiB is allocated by PyTorch, and 6.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)