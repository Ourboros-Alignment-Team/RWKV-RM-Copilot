[2025-04-09 02:36:44,733] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2025-04-09 02:36:44,733] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2025-04-09 02:36:44,733] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2025-04-09 02:36:44,733] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2025-04-09 02:36:44,733] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
Epoch 0, Loss: 2.0, Positive Score: 0.0, Negative Score: 0.0
/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1829: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
Epoch 0, Loss: 2.0, Positive Score: 0.0, Negative Score: 0.0
Epoch 0, Loss: 2.0, Positive Score: 0.0, Negative Score: 0.0
Epoch 0, Loss: 2.0, Positive Score: 0.0, Negative Score: 0.0
Epoch 0, Loss: 2.0, Positive Score: 0.0, Negative Score: 0.0
Epoch 0, Loss: 2.0, Positive Score: 0.0, Negative Score: 0.0
Epoch 0, Loss: 2.0, Positive Score: 0.0, Negative Score: 0.0
Epoch 0, Loss: 2.0, Positive Score: 0.0, Negative Score: 0.0
Epoch 1, Loss: 2.0, Positive Score: 0.0, Negative Score: 0.0
Traceback (most recent call last):
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/train_model.py", line 242, in <module>
    model_engine.backward(Loss)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1862, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1901, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/function.py", line 289, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 658, in backward
    outputs = ctx.run_function(*detached_inputs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/model/block.py", line 27, in forward
    x_attn, v_first = self.att(
                      ^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/model/att.py", line 160, in forward
    x = self.ln_x(x.view(B * T, C)).view(B, T, C)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/modules/normalization.py", line 287, in forward
    return F.group_norm(
           ^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/nn/functional.py", line 2561, in group_norm
    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 268.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 158.81 MiB is free. Including non-PyTorch memory, this process has 23.33 GiB memory in use. Of the allocated memory 22.57 GiB is allocated by PyTorch, and 266.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)