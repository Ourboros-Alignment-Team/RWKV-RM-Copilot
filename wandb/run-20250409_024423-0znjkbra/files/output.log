[2025-04-09 02:44:33,975] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2025-04-09 02:44:33,975] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2025-04-09 02:44:33,975] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2025-04-09 02:44:33,975] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2025-04-09 02:44:33,975] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
Epoch 0, Loss: 0.0234375, Positive Score: -0.5, Negative Score: -0.5234375
/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1829: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
Epoch 0, Loss: -0.01953125, Positive Score: -0.53125, Negative Score: -0.51171875
Epoch 0, Loss: -0.01171875, Positive Score: -0.53125, Negative Score: -0.51953125
Epoch 0, Loss: 0.0, Positive Score: -0.5078125, Negative Score: -0.5078125
Epoch 0, Loss: -0.01953125, Positive Score: -0.51953125, Negative Score: -0.5
Epoch 0, Loss: 0.03515625, Positive Score: -0.5078125, Negative Score: -0.54296875
Epoch 0, Loss: -0.00390625, Positive Score: -0.52734375, Negative Score: -0.5234375
Epoch 0, Loss: 0.00390625, Positive Score: -0.5234375, Negative Score: -0.52734375
Epoch 0, Loss: 0.00390625, Positive Score: -0.50390625, Negative Score: -0.5078125
Epoch 0, Loss: 0.01171875, Positive Score: -0.53125, Negative Score: -0.54296875
[2025-04-09 02:45:52,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:45:52,462] [INFO] [timer.py:215:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.13486155815753653, CurrSamplesPerSec=0.12234233887363366, MemAllocated=5.52GB, MaxMemAllocated=19.12GB
Epoch 0, Loss: -0.015625, Positive Score: -0.546875, Negative Score: -0.53125
Epoch 1, Loss: -0.0234375, Positive Score: -0.546875, Negative Score: -0.5234375
Epoch 1, Loss: -0.04296875, Positive Score: -0.55859375, Negative Score: -0.515625
Epoch 1, Loss: 0.00390625, Positive Score: -0.5234375, Negative Score: -0.52734375
Epoch 1, Loss: -0.00390625, Positive Score: -0.52734375, Negative Score: -0.5234375
Epoch 1, Loss: 0.0078125, Positive Score: -0.53125, Negative Score: -0.5390625
Epoch 1, Loss: 0.015625, Positive Score: -0.5, Negative Score: -0.515625
Epoch 1, Loss: 0.00390625, Positive Score: -0.51953125, Negative Score: -0.5234375
Epoch 1, Loss: -0.0234375, Positive Score: -0.53125, Negative Score: -0.5078125
Epoch 1, Loss: 0.04296875, Positive Score: -0.53515625, Negative Score: -0.578125
[2025-04-09 02:47:09,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=20, lr=[1e-05, 1e-05], mom=[(0.9, 0.99), (0.9, 0.99)]
[2025-04-09 02:47:09,053] [INFO] [timer.py:215:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.13258546888186412, CurrSamplesPerSec=0.15399321091160714, MemAllocated=5.52GB, MaxMemAllocated=19.37GB
Epoch 1, Loss: -0.01171875, Positive Score: -0.53515625, Negative Score: -0.5234375
Epoch 1, Loss: -0.046875, Positive Score: -0.56640625, Negative Score: -0.51953125
Traceback (most recent call last):
  File "/home/li/MachineLr/RWKV-Development-Tools-ssg/wkv7_reward_model/train_model.py", line 240, in <module>
    model_engine.backward(Loss)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1862, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1901, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/function.py", line 289, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 684, in backward
    torch.autograd.backward(output_tensors, grad_tensors)
  File "/home/li/anaconda3/envs/LLMs/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 762.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 682.81 MiB is free. Including non-PyTorch memory, this process has 22.83 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 530.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)